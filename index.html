<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Comprehensive Machine Learning Guide</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" xintegrity="sha384-n8MVd4RsNIU0KOVEMVIROPtCIDJcpcDTAOT9sWlLplmg7JYTT7GoqxSrg3odEIFG" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" xintegrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" xintegrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
    <!-- Chosen Palette: Academic Calm -->
    <!-- Application Structure Plan: A single, comprehensive, scrollable HTML page designed to function like a detailed PDF. The structure is thematic, with a table of contents at the top for easy navigation via anchor links. This format is chosen to present a large volume of detailed theoretical information and formulas in a linear, organized manner, which is ideal for in-depth study and review, directly addressing the user's preference for a content-rich, document-style layout over an interactive SPA. -->
    <!-- Visualization & Content Choices: 
        - Report Info: All formulas, notations, theoretical points, and PYQ insights. Goal: Inform, Organize, Synthesize. Viz/Presentation Method: Structured text sections with headings, bullet points, tables, and KaTeX for clear mathematical rendering. Interaction: Anchor links for navigation. Justification: This structure provides a complete and exhaustive reference guide, prioritizing content depth and clarity over interactivity, as requested.
        - Report Info: Visual concepts (e.g., loss function shapes). Goal: Illustrate. Viz/Presentation Method: Static images embedded where necessary to support textual explanations. Justification: Static images are sufficient and more direct for illustrating concepts within a document-style format.
        - Library/Method: KaTeX for math rendering, Vanilla JS for smooth scrolling. -->
    <!-- CONFIRMATION: NO SVG graphics used. NO Mermaid JS used. -->
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #F8F7F4;
            color: #374151;
        }
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap');
        .katex { font-size: 1.1em !important; }
        h2 {
            border-bottom: 2px solid #6366F1;
            padding-bottom: 0.5rem;
            margin-top: 2.5rem;
            margin-bottom: 1.5rem;
        }
        h3 {
            margin-top: 2rem;
            margin-bottom: 1rem;
            border-left: 4px solid #818CF8;
            padding-left: 1rem;
        }
        .content-section {
            background-color: white;
            padding: 2rem;
            border-radius: 0.5rem;
            box-shadow: 0 4px 6px -1px rgb(0 0 0 / 0.1), 0 2px 4px -2px rgb(0 0 0 / 0.1);
            margin-bottom: 2rem;
        }
        .toc a {
            transition: color 0.2s;
        }
        .toc a:hover {
            color: #4F46E5;
        }
    </style>
</head>
<body class="antialiased">

    <div class="container mx-auto p-4 md:p-8 max-w-5xl">
        <header class="text-center mb-12">
            <h1 class="text-5xl font-bold text-indigo-800">MLT(WEEK 1-12 revision Notes)</h1>
            <p class="text-xl text-gray-600 mt-4">An exhaustive reference of formulas, theories, and insights from all provided materials.</p>
        </header>

        <!-- Table of Contents -->
        <div class="content-section toc">
            <h2 class="text-3xl font-bold text-gray-800 !mt-0">Table of Contents</h2>
            <ul class="list-disc list-inside space-y-2 text-lg">
                <li><a href="#unsupervised"><strong>Part 1:</strong> Unsupervised Learning (PCA, K-Means, GMM)</a></li>
                <li><a href="#estimation"><strong>Part 2:</strong> Parameter Estimation (MLE, MAP)</a></li>
                <li><a href="#supervised"><strong>Part 3:</strong> Supervised Learning (Regression, Classification, SVM, Perceptron)</a></li>
                <li><a href="#ensemble"><strong>Part 4:</strong> Ensemble Methods & Neural Networks</a></li>
                <li><a href="#pyq-theory"><strong>Part 5:</strong> Important Theoretical Points from PYQs</a></li>
            </ul>
        </div>

        <!-- Part 1: Unsupervised Learning -->
        <section id="unsupervised" class="content-section">
            <h2 class="text-3xl font-bold text-gray-800">Part 1: Unsupervised Learning</h2>
            
            <h3 class="text-2xl font-semibold text-gray-700">1. Principal Component Analysis (PCA)</h3>
            <p class="mb-4">A technique for <strong>dimensionality reduction</strong> that finds orthogonal directions of maximum variance in the data.</p>
            
            <h4 class="text-xl font-semibold mt-6 mb-2">Notation Meaning</h4>
            <div class="overflow-x-auto"><table class="w-full text-left border-collapse"><thead><tr class="bg-indigo-50"><th class="p-2 border">Symbol</th><th class="p-2 border">Meaning</th></tr></thead><tbody><tr><td class="p-2 border">$$d, n$$</td><td class="p-2 border">Number of features, Number of data points</td></tr><tr><td class="p-2 border">$$x_i, \overline{x}$$</td><td class="p-2 border">i-th data point, Mean vector of dataset</td></tr><tr><td class="p-2 border">$$X$$</td><td class="p-2 border">Centered data matrix ($$d \times n$$)</td></tr><tr><td class="p-2 border">$$C$$</td><td class="p-2 border">Covariance matrix ($$d \times d$$)</td></tr><tr><td class="p-2 border">$$K$$</td><td class="p-2 border">Gram matrix ($$n \times n$$)</td></tr><tr><td class="p-2 border">$$w_j, \lambda_j$$</td><td class="p-2 border">j-th Principal Component (eigenvector of C), j-th eigenvalue</td></tr></tbody></table></div>

            <h4 class="text-xl font-semibold mt-6 mb-2">Core Concepts & Formulas</h4>
            <ul class="list-disc list-inside space-y-3">
                <li><strong>Centering:</strong> $$x_{i}' = x_{i} - \overline{x}$$, where $$\\overline{x} = \frac{1}{n}\sum_{i=1}^{n}x_{i}$$</li>
                <li><strong>Covariance Matrix:</strong> $$C = \frac{1}{n}XX^T$$</li>
                <li><strong>Gram Matrix:</strong> $$K = X^T X$$</li>
                <li><strong>Relationship:</strong> $$C$$ and $$K/n$$ share the same non-zero eigenvalues. The PCs ($$w_j$$) can be recovered from eigenvectors of K ($$v_j$$) via: $$w_j = \frac{X v_j}{\sqrt{\lambda_j}}$$.</li>
                <li><strong>Optimization (Maximizing Variance):</strong> $$\max_{w: ||w||=1} w^T C w = \lambda_1$$</li>
                <li><strong>Optimization (Minimizing Reconstruction Error):</strong> $$\min_{W \in \mathbb{R}^{d \times k}} \frac{1}{n} \sum_{i=1}^{n} ||x_i - WW^T x_i||^2$$</li>
                <li><strong>Projection (New Coordinates):</strong> $$X' = W^T X$$</li>
                <li><strong>Reconstruction:</strong> $$X_{recon} = WW^T X$$</li>
                <li><strong>Reconstruction Error:</strong> $$\frac{1}{n}\sum||x_i - x_{i,recon}||^2 = \sum_{j=k+1}^{d}\lambda_j$$</li>
                <li><strong>Total Variance:</strong> $$\text{Trace}(C) = \sum_{j=1}^{d}\lambda_j$$</li>
                <li><strong>Proportion of Variance Captured:</strong> $$\frac{\sum_{j=1}^{k}\lambda_j}{\sum_{j=1}^{d}\lambda_j}$$</li>
            </ul>

            <h3 class="text-2xl font-semibold text-gray-700">2. K-Means Clustering</h3>
            <p class="mb-4">An iterative algorithm that partitions a dataset into K pre-defined, non-overlapping clusters.</p>
            <h4 class="text-xl font-semibold mt-6 mb-2">Core Concepts & Formulas</h4>
             <ul class="list-disc list-inside space-y-3">
                <li><strong>Objective Function (WCSS):</strong> $$J = \sum_{k=1}^{K} \sum_{x_i \in C_k} ||x_i - \mu_k||^2$$</li>
                <li><strong>Assignment Step:</strong> $$z_i = \text{argmin}_k ||x_i - \mu_k||^2$$</li>
                <li><strong>Update Step:</strong> $$\mu_k = \frac{1}{|C_k|} \sum_{x_i \in C_k} x_i$$</li>
                <li><strong>K-Means++ Initialization:</strong> A smarter way to initialize centroids to avoid poor local minima.
                    <ol class="list-decimal list-inside ml-4 mt-2">
                        <li>Choose first centroid $$\mu_1$$ uniformly at random from data points.</li>
                        <li>For each subsequent centroid, choose point $$x_i$$ with a probability proportional to its squared distance to the <em>nearest existing</em> centroid: $$P(x_i) \propto \min_{j=1,...,k_{chosen}} ||x_i - \mu_j||^2$$</li>
                    </ol>
                </li>
            </ul>

            <h3 class="text-2xl font-semibold text-gray-700">3. Gaussian Mixture Models (GMM) & EM Algorithm</h3>
            <p class="mb-4">A probabilistic model that assumes data is generated from a mixture of several Gaussian distributions. It performs "soft" clustering.</p>
            <h4 class="text-xl font-semibold mt-6 mb-2">Core Concepts & Formulas</h4>
            <ul class="list-disc list-inside space-y-3">
                <li><strong>GMM Probability Density Function (PDF):</strong> $$f(x;\theta)=\sum_{k=1}^{K}\pi_{k}\mathcal{N}(x;\mu_{k},\Sigma_{k})$$
                    <ul class="list-circle list-inside ml-4 mt-2">
                        <li>$$\pi_k$$: Mixture probability (weight) of component k.</li>
                        <li>$$\mathcal{N}(x;\mu_{k},\Sigma_{k})$$: PDF of the k-th Gaussian component.</li>
                    </ul>
                </li>
                <li><strong>Expectation-Maximization (EM) Algorithm:</strong> An iterative method to find the MLE or MAP estimates for parameters of latent variable models like GMM.
                    <ul class="list-circle list-inside ml-4 mt-2">
                        <li><strong>E-Step (Expectation):</strong> Calculate the "responsibilities" or posterior probability of component k for data point i.
                        $$\lambda_k^i = P(z_i=k | x_i; \theta) = \frac{\pi_k \mathcal{N}(x_i | \mu_k, \Sigma_k)}{\sum_{j=1}^{K} \pi_j \mathcal{N}(x_i | \mu_j, \Sigma_j)}$$</li>
                        <li><strong>M-Step (Maximization):</strong> Update the model parameters using the responsibilities.
                            <p class="mt-2">Total responsibility for cluster k: $$N_k = \sum_{i=1}^{n} \lambda_k^i$$</p>
                            <p>New mixture weight: $$\pi_k^{new} = \frac{N_k}{n}$$</p>
                            <p>New mean: $$\mu_k^{new} = \frac{1}{N_k} \sum_{i=1}^{n} \lambda_k^i x_i$$</p>
                            <p>New covariance: $$\Sigma_k^{new} = \frac{1}{N_k} \sum_{i=1}^{n} \lambda_k^i (x_i - \mu_k^{new})(x_i - \mu_k^{new})^T$$</p>
                        </li>
                    </ul>
                </li>
            </ul>
        </section>

        <!-- Part 2: Parameter Estimation -->
        <section id="estimation" class="content-section">
            <h2 class="text-3xl font-bold text-gray-800">Part 2: Parameter Estimation</h2>
            <h3 class="text-2xl font-semibold text-gray-700">4. MLE and Bayesian (MAP) Estimation</h3>
            <h4 class="text-xl font-semibold mt-6 mb-2">Maximum Likelihood Estimation (MLE)</h4>
            <ul class="list-disc list-inside space-y-3">
                <li><strong>Likelihood:</strong> $$L(\theta; D) = \prod_{i=1}^{n} P(x_i; \theta)$$</li>
                <li><strong>Log-Likelihood:</strong> $$l(\theta; D) = \sum_{i=1}^{n} \log P(x_i; \theta)$$</li>
                <li><strong>MLE for Bernoulli:</strong> For $$n_h$$ heads and $$n_t$$ tails, $$\hat{p}_{MLE} = \frac{n_h}{n_h + n_t}$$</li>
                 <li><strong>MLE vs. MSE:</strong> The MLE estimator for weights in a linear model ($$y = w^Tx + \epsilon, \epsilon \sim \mathcal{N}(0, \sigma^2)$$) is unbiased, meaning on average it will be correct. However, it may not always have the lowest Mean Squared Error (MSE) compared to a biased estimator like MAP.</li>
            </ul>
            <h4 class="text-xl font-semibold mt-6 mb-2">Bayesian Estimation (MAP)</h4>
            <ul class="list-disc list-inside space-y-3">
                <li><strong>Bayes' Theorem for Parameters:</strong> $$P(\theta|D) = \frac{P(D|\theta)P(\theta)}{P(D)} \propto \text{Likelihood} \times \text{Prior}$$</li>
                <li><strong>Bernoulli with Beta Prior:</strong> If prior is $$Beta(\alpha, \beta)$$ and data has $$n_h$$ heads, $$n_t$$ tails:
                    <ul class="list-circle list-inside ml-4 mt-2">
                        <li><strong>Posterior Distribution:</strong> $$P(p|D) = Beta(\alpha + n_h, \beta + n_t)$$</li>
                        <li><strong>Posterior Mean:</strong> $$E[p|D] = \frac{\alpha + n_h}{\alpha + \beta + n_h + n_t}$$</li>
                        <li><strong>MAP Estimate:</strong> $$\hat{p}_{MAP} = \frac{\alpha + n_h - 1}{\alpha + \beta + n_h + n_t - 2}$$</li>
                    </ul>
                </li>
                 <li><strong>MAP vs. MSE:</strong> The MAP estimator is generally biased because it incorporates prior information. This bias can sometimes lead to a **lower MSE** than the unbiased MLE, especially with small datasets, because it reduces the estimator's variance.</li>
            </ul>
        </section>

        <!-- Part 3: Supervised Learning -->
        <section id="supervised" class="content-section">
            <h2 class="text-3xl font-bold text-gray-800">Part 3: Supervised Learning</h2>
            <h3 class="text-2xl font-semibold text-gray-700">5. Regression Models</h3>
            <h4 class="text-xl font-semibold mt-6 mb-2">Linear Regression</h4>
            <ul class="list-disc list-inside space-y-3">
                <li><strong>Model:</strong> $$\hat{y} = w^T x + b$$</li>
                <li><strong>Loss (SSE):</strong> $$L(w) = \frac{1}{2} ||X^T w - y||^2$$</li>
                <li><strong>Gradient of SSE:</strong> $$\nabla_w L(w) = XX^T w - Xy$$</li>
                <li><strong>Normal Equations (Solution):</strong> $$\hat{w} = (XX^T)^{-1}Xy$$</li>
            </ul>
            <h4 class="text-xl font-semibold mt-6 mb-2">Regularization</h4>
            <ul class="list-disc list-inside space-y-3">
                <li><strong>Ridge (L2) Loss:</strong> $$L_{ridge}(w) = \text{SSE} + \frac{\lambda}{2} ||w||_2^2$$</li>
                <li><strong>Ridge Solution:</strong> $$\hat{w}_{ridge} = (XX^T + \lambda I)^{-1}Xy$$</li>
                 <li><strong>Probabilistic View of Ridge:</strong> Ridge regression is equivalent to finding the MAP estimate of $$w$$ under the assumptions:
                    <ul class="list-circle list-inside ml-4 mt-2">
                        <li>Likelihood: $$y | x \sim \mathcal{N}(w^T x, \sigma^2 I)$$</li>
                        <li>Prior: $$w \sim \mathcal{N}(0, \gamma^2 I)$$</li>
                        <li>Under this setup, the regularization parameter $$\lambda = \frac{\sigma^2}{\gamma^2}$$ (i.e., noise variance / prior variance).</li>
                    </ul>
                </li>
                <li><strong>LASSO (L1) Loss:</strong> $$L_{LASSO}(w) = \text{SSE} + \lambda ||w||_1$$</li>
            </ul>

            <h3 class="text-2xl font-semibold text-gray-700">6. Classification Models</h3>
             <h4 class="text-xl font-semibold mt-6 mb-2">Decision Trees</h4>
            <ul class="list-disc list-inside space-y-3">
                <li><strong>Entropy (Binary):</strong> $$E(p) = -p \log_2(p) - (1-p) \log_2(1-p)$$ where $$p$$ is the proportion of positive samples.</li>
                <li><strong>Information Gain (IG):</strong> $$IG = E_{parent} - [\gamma E_{left} + (1-\gamma) E_{right}]$$ where $$\gamma = \frac{n_{left}}{n_{parent}}$$</li>
            </ul>
            <h4 class="text-xl font-semibold mt-6 mb-2">Perceptron Algorithm</h4>
            <ul class="list-disc list-inside space-y-3">
                <li><strong>Model:</strong> $$\hat{y} = \text{sign}(w^T x)$$</li>
                <li><strong>Update Rule:</strong> If a mistake is made (i.e., $$y_i(w^T x_i) \le 0$$), then $$w \leftarrow w + y_i x_i$$.</li>
                <li><strong>Convergence Theorem:</strong> For a linearly separable dataset with margin $$\gamma$$, where all points lie within a ball of radius $$R$$, the Perceptron algorithm is guaranteed to converge.</li>
                <li><strong>Mistake Bound:</strong> The maximum number of mistakes (t) is bounded by: $$t \le \left(\frac{R}{\gamma}\right)^2$$. This is derived from:
                    <ul class="list-circle list-inside ml-4 mt-2">
                        <li><strong>Upper Bound on $$||w||^2$$:</strong> Each mistake increases the squared norm by at most $$R^2$$. After t mistakes, $$||w^{(t)}||^2 \le t R^2$$.</li>
                        <li><strong>Lower Bound on $$||w||^2$$:</strong> Each mistake increases the alignment with the optimal separator $$w^*$$ by at least $$\gamma$$. After t mistakes, $$||w^{(t)}||^2 \ge (t\gamma)^2$$.</li>
                        <li><strong>Combining Bounds:</strong> $$(t\gamma)^2 \le ||w^{(t)}||^2 \le t R^2 \implies t^2\gamma^2 \le tR^2 \implies t \le \frac{R^2}{\gamma^2}$$.</li>
                    </ul>
                </li>
            </ul>
            <h4 class="text-xl font-semibold mt-6 mb-2">Support Vector Machines (SVM)</h4>
            <ul class="list-disc list-inside space-y-3">
                <li><strong>Functional Margin:</strong> $$\hat{\gamma}_i = y_i(w^T x_i + b)$$</li>
                <li><strong>Geometric Margin:</strong> $$\gamma_i = \frac{y_i(w^T x_i + b)}{||w||}$$</li>
                <li><strong>Hard-Margin Primal:</strong> $$\min_w \frac{1}{2}||w||^2 \quad \text{s.t.} \quad y_i(w^T x_i + b) \ge 1$$</li>
                <li><strong>Soft-Margin Primal:</strong> $$\min_{w, \xi} \frac{1}{2}||w||^2 + C \sum \xi_i \quad \text{s.t.} \quad y_i(w^T x_i + b) \ge 1 - \xi_i, \xi_i \ge 0$$</li>
                <li><strong>Dual Objective:</strong> $$\max_{0 \le \alpha_i \le C} \sum \alpha_i - \frac{1}{2} \sum_i \sum_j \alpha_i \alpha_j y_i y_j k(x_i, x_j)$$</li>
                <li><strong>Weight Vector from Dual:</strong> $$w^* = \sum_{i=1}^{n} \alpha_i^* y_i x_i$$</li>
                <li><strong>Prediction with Kernels:</strong> $$\hat{y}_{test} = \text{sign}\left(\sum_{i \in SVs} \alpha_i^* y_i k(x_i, x_{test}) + b\right)$$</li>
            </ul>
            <h4 class="text-xl font-semibold mt-6 mb-2">Margin Calculation Explained (Hard Margin SVM)</h4>
            <ul class="list-disc list-inside space-y-3">
                <li><strong>Distance from a point $$x_i$$ to hyperplane $$w^Tx+b=0$$:</strong> $$\text{dist}(x_i) = \frac{|w^Tx_i + b|}{||w||}$$</li>
                <li><strong>SVM Constraint:</strong> For all points, $$y_i(w^Tx_i + b) \ge 1$$.</li>
                <li><strong>Support Vector Condition:</strong> For the closest points (support vectors), the constraint is an equality: $$y_i(w^Tx_i + b) = 1$$. This also means $$|w^Tx_i + b| = 1$$.</li>
                <li><strong>Distance to Support Vector:</strong> Plugging this into the distance formula gives the distance from the decision boundary to the closest point: $$\text{dist}_{SV} = \frac{1}{||w||}$$.</li>
                <li><strong>Total Margin Width:</strong> The total gap between the two supporting hyperplanes ($$w^Tx+b=1$$ and $$w^Tx+b=-1$$) is twice this distance: $$\text{Total Margin} = \frac{2}{||w||}$$.</li>
            </ul>
            <h4 class="text-xl font-semibold mt-6 mb-2">Conditions for Support Vectors in Soft-Margin SVM</h4>
            <div class="overflow-x-auto"><table class="w-full text-left border-collapse"><thead><tr class="bg-indigo-50"><th class="p-2 border">Case</th><th class="p-2 border">Primal Condition</th><th class="p-2 border">Dual Condition</th><th class="p-2 border">Slack ($$\xi_i$$)</th><th class="p-2 border">Meaning</th></tr></thead><tbody><tr><td class="p-2 border">Outside Margin</td><td class="p-2 border">$$(w^T x_i)y_i > 1$$</td><td class="p-2 border">$$\alpha_i^* = 0$$</td><td class="p-2 border">$$\xi_i = 0$$</td><td class="p-2 border">Not a support vector</td></tr><tr><td class="p-2 border">On Margin</td><td class="p-2 border">$$(w^T x_i)y_i = 1$$</td><td class="p-2 border">$$\alpha_i^* \in (0, C)$$</td><td class="p-2 border">$$\xi_i = 0$$</td><td class="p-2 border">Support vector</td></tr><tr><td class="p-2 border">Within Margin</td><td class="p-2 border">$$1 > (w^T x_i)y_i > 0$$</td><td class="p-2 border">$$\alpha_i^* = C$$</td><td class="p-2 border">$$0 < \xi_i < 1$$</td><td class="p-2 border">Support vector</td></tr><tr><td class="p-2 border">Misclassified</td><td class="p-2 border">$$(w^T x_i)y_i < 0$$</td><td class="p-2 border">$$\alpha_i^* = C$$</td><td class="p-2 border">$$\xi_i > 1$$</td><td class="p-2 border">Support vector</td></tr></tbody></table></div>
        </section>

        <!-- Part 4: Ensemble Methods -->
        <section id="ensemble" class="content-section">
            <h2 class="text-3xl font-bold text-gray-800">Part 4: Ensemble Methods & Neural Networks</h2>
            <h3 class="text-2xl font-semibold text-gray-700">7. Ensemble Learning</h3>
            <h4 class="text-xl font-semibold mt-6 mb-2">Bagging (Bootstrap Aggregating)</h4>
            <ul class="list-disc list-inside space-y-3">
                <li><strong>Goal:</strong> Reduce variance.</li>
                <li><strong>Method:</strong> Trains deep models in parallel on bootstrap samples.</li>
                <li><strong>Probability of a point in a bag:</strong> $$1 - (1 - 1/n)^n \approx 1 - 1/e \approx 0.632$$</li>
            </ul>
             <h4 class="text-xl font-semibold mt-6 mb-2">Boosting (AdaBoost)</h4>
            <ul class="list-disc list-inside space-y-3">
                <li><strong>Goal:</strong> Reduce bias.</li>
                <li><strong>Method:</strong> Trains weak learners sequentially.</li>
                <li><strong>Classifier Weight:</strong> $$\alpha_t = \frac{1}{2} \ln(\frac{1 - \epsilon_t}{\epsilon_t})$$</li>
                <li><strong>Data Weight Update:</strong> $$D_{t+1}(i) \propto D_t(i) \exp(-\alpha_t y_i h_t(x_i))$$</li>
                <li><strong>Final Classifier:</strong> $$H(x) = \text{sign}\left(\sum_{t=1}^{T} \alpha_t h_t(x)\right)$$</li>
            </ul>
        </section>

        <!-- Part 5: PYQ Theory -->
        <section id="pyq-theory" class="content-section">
            <h2 class="text-3xl font-bold text-gray-800">Part 5: Important Theoretical Points from PYQs</h2>
             <ul class="list-disc list-inside space-y-4 text-lg">
                <li><strong>Bias-Variance Tradeoff:</strong> High bias models (e.g., linear regression on non-linear data) underfit. High variance models (e.g., deep decision trees) overfit. The goal is to find a balance for the lowest test error.</li>
                <li><strong>Model Complexity vs. Error:</strong> As complexity increases, training error decreases, while test error forms a U-shape.</li>
                <li><strong>Regularization:</strong> Its main purpose is to penalize large coefficients to reduce overfitting and model variance. In practice, $$\lambda$$ is a tuning knob found via cross-validation.</li>
                <li><strong>Ensemble Methods:</strong> Bagging reduces variance using complex models in parallel. Boosting reduces bias using weak models sequentially.</li>
                <li><strong>SVMs:</strong> A point is a support vector only if its dual variable $$\alpha_i^* > 0$$. For linearly separable data, soft-margin and hard-margin SVMs produce the same result.</li>
             </ul>
        </section>

    </div>

    <footer class="text-center p-4 mt-8 bg-gray-100 border-t border-gray-200">
        <p class="text-gray-600">BY ~KarTiK🍁(NOTE:Made with the help of AI)</p>
    </footer>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false}
                ]
            });

            document.querySelectorAll('.toc a').forEach(anchor => {
                anchor.addEventListener('click', function (e) {
                    e.preventDefault();
                    document.querySelector(this.getAttribute('href')).scrollIntoView({
                        behavior: 'smooth'
                    });
                });
            });
        });
    </script>
</body>
</html>
